{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_path(model_name, dataset_name, model_type):\n",
    "    current_dir = os.getcwd()\n",
    "    model_subdir = \"model\"\n",
    "    model_save_dir = os.path.join(current_dir, model_subdir)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    model_base_filename = f\"best_{model_name}_on_{dataset_name}_{model_type}.pth\"\n",
    "    model_save_path = os.path.join(model_save_dir, model_base_filename)\n",
    "\n",
    "    return model_save_path\n",
    "\n",
    "def append_metrics_to_log(log_file_path, epoch, train_loss, val_loss=None, val_accuracy=None, val_mae=None):\n",
    "    log_dir = os.path.dirname(log_file_path)\n",
    "\n",
    "    if log_dir and not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    write_header = not os.path.exists(log_file_path)\n",
    "\n",
    "    header = ['Epoch', 'Train Loss']\n",
    "    row_data = [epoch, f\"{float(train_loss):.4f}\"]\n",
    "\n",
    "    if val_loss is not None:\n",
    "        header.append('Validation Loss')\n",
    "        row_data.append(f\"{float(val_loss):.4f}\")\n",
    "\n",
    "    if val_accuracy is not None:\n",
    "        header.append('Validation Accuracy')\n",
    "        row_data.append(f\"{float(val_accuracy)*100:.2f}%\")\n",
    "\n",
    "    if val_mae is not None:\n",
    "        header.append('Validation MAE')\n",
    "        row_data.append(f\"{float(val_mae):.4f}\")\n",
    "\n",
    "    with open(log_file_path, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_imdb(data, pad_value=50257, use_embedding=True):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sequences = [x[0] for x in data]\n",
    "    scores = torch.tensor([x[1] for x in data], dtype=torch.float32)\n",
    "    labels = torch.tensor([x[2] for x in data], dtype=torch.float32)\n",
    "\n",
    "    original_seq_lengths = torch.tensor([len(s) for s in sequences], dtype=torch.long)\n",
    "    padded_seqs_long = pad_sequence(sequences, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    if use_embedding:\n",
    "        return padded_seqs_long, original_seq_lengths, scores, labels\n",
    "    else:\n",
    "        padded_seqs_float = padded_seqs_long.unsqueeze(-1).float()\n",
    "        packed_input = pack_padded_sequence(padded_seqs_float, original_seq_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        return packed_input, scores, labels\n",
    "    \n",
    "\n",
    "def collate_fn_mnist(x):\n",
    "    images = [i[0] for i in x]\n",
    "    labels = [i[1] for i in x]\n",
    "\n",
    "    images = torch.vstack(images)\n",
    "    images = images.squeeze(1)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return images,labels\n",
    "\n",
    "def tokenize_text(text_list, tokenizer):\n",
    "    tokenized_text = []\n",
    "    for text in text_list:\n",
    "        tokens = torch.tensor(tokenizer.encode(text))\n",
    "        tokenized_text.append(tokens)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, comments_token_ids, sentiments, scores):\n",
    "        self.comments_token_ids = comments_token_ids\n",
    "        self.sentiments = sentiments\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments_token_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.comments_token_ids[idx], self.scores[idx], self.sentiments[idx]\n",
    "    \n",
    "def create_IMDB_dataloader(dataset, batch_size=32, shuffle=True, num_workers=0, use_embedding=True, pad_value=50257):\n",
    "    collate_wrapper = lambda x: collate_fn_imdb(x, pad_value=pad_value, use_embedding=use_embedding)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_wrapper)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1326,), (0.3106,)),\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='data', train=True, download=True, transform=train_transform)\n",
    "mnist_test = datasets.MNIST(root='data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_size = int(0.9 * len(mnist_train))\n",
    "val_size = len(mnist_train) - train_size\n",
    "mnist_train_set, mnist_val_set = random_split(mnist_train, [train_size, val_size])\n",
    "\n",
    "sample, target = mnist_train[0]\n",
    "print(f\"Sample count: {len(mnist_train)}\")\n",
    "print(f\"Sample shape: {sample.shape}, Target: {target}\")\n",
    "print(f\"Sample type: {sample.dtype}, Target type: {type(target)}\")\n",
    "print(f\"Sample min: {sample.min()}, Sample max: {sample.max()}\")\n",
    "print(f\"Sample training set size: {len(mnist_train_set)}\")\n",
    "print(f\"Sample validation set size: {len(mnist_val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8347874",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train = pd.read_csv(\"data/IMDB_train.csv\")\n",
    "IMDB_test = pd.read_csv(\"data/IMDB_test.csv\")\n",
    "\n",
    "train_comments = IMDB_train[\"preprocessed_comments\"].to_list()\n",
    "train_sentiments = IMDB_train[\"sentiment\"].to_list()\n",
    "train_scores = IMDB_train[\"score\"].to_list()\n",
    "test_comments = IMDB_test[\"preprocessed_comments\"].to_list()\n",
    "test_sentiments = IMDB_test[\"sentiment\"].to_list()\n",
    "test_scores = IMDB_test[\"score\"].to_list()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenized_train_comments = tokenize_text(train_comments, tokenizer)\n",
    "tokenized_test_comments = tokenize_text(test_comments, tokenizer)\n",
    "sample = train_comments[0]\n",
    "sample_sentiment = train_sentiments[0]\n",
    "sample_score = train_scores[0]\n",
    "token_ids = tokenized_train_comments[0]\n",
    "reconstructed = tokenizer.decode(token_ids.tolist())\n",
    "\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Sentiment: {sample_sentiment}\")\n",
    "print(f\"Score: {sample_score}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Reconstructed: {reconstructed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_RNN_MNIST(nn.Module):\n",
    "    def __init__(self, input_size=28, out_dim=10):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, out_dim, batch_first=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1, x.size(0), 10).to(x.device)\n",
    "\n",
    "        output, hn = self.rnn(x, h0)\n",
    "\n",
    "        return output[:,-1,:]\n",
    "    \n",
    "class Basic_LSTM_MNIST(nn.Module):\n",
    "    def __init__(self, input_size=28, out_dim=10):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, out_dim, batch_first=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1, x.size(0), 10).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), 10).to(x.device)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(x, (h0,c0))\n",
    "\n",
    "        return output[:,-1,:]\n",
    "    \n",
    "class Basic_GRU_MNIST(nn.Module):\n",
    "    def __init__(self, input_size=28, out_dim=10):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, out_dim, batch_first=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1, x.size(0), 10).to(x.device)\n",
    "\n",
    "        output, hn = self.gru(x, h0)\n",
    "\n",
    "        return output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hymmn0s_RNN_MNIST(nn.Module):\n",
    "    def __init__(self, input_size=28, hiddin_dim=14, out_dim=10, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hiden_dim = hiddin_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hiddin_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hiddin_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, fn = self.rnn(x)\n",
    "\n",
    "        return self.fc(output[:,-1,:])\n",
    "    \n",
    "class Hymmn0s_LSTM_MNIST(nn.Module):\n",
    "    def __init__(self, input_size=28, hiddin_dim=14, out_dim=10, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hiden_dim = hiddin_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hiddin_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hiddin_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        return self.fc(output[:,-1,:])\n",
    "    \n",
    "class Hymmn0s_GRU_MNIST(nn.Module):\n",
    "    def __init__(self, input_size=28, hiddin_dim=14, out_dim=10, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hiden_dim = hiddin_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hiddin_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hiddin_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hn = self.gru(x)\n",
    "\n",
    "        return self.fc(output[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd260572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(model, train_dataset, val_dataset, model_name, dataset_name, model_type, epochs = 50):\n",
    "    lr = 2e-3\n",
    "    batch = 256\n",
    "    weight_decay = 2e-4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, collate_fn=collate_fn_mnist)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False, collate_fn=collate_fn_mnist)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    model_save_path = get_save_path(model_name, dataset_name, model_type)\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader)}\")\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                output = model(x)\n",
    "                loss = loss_fn(output, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "            \n",
    "            if correct / total > best_val_accuracy:\n",
    "                best_val_accuracy = correct / total\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Model saved with accuracy: {best_val_accuracy}\")\n",
    "            \n",
    "            append_metrics_to_log(f\"log/{model_name}_on_{dataset_name}_training_log_{model_type}.csv\", epoch, train_loss/len(train_loader), val_loss/len(val_loader), correct / total)\n",
    "\n",
    "            print(f\"Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "\n",
    "def test_mnist(model, test_dataset):\n",
    "    batch = 256\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch, shuffle=False, collate_fn=collate_fn_mnist)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baa96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_mnist = Basic_RNN_MNIST(input_size=28, out_dim=10).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_mnist(rnn_mnist, mnist_train_set, mnist_val_set, \"RNN\", \"MNIST\",\"Basic\")\n",
    "rnn_mnist.load_state_dict(torch.load(get_save_path(\"RNN\", \"MNIST\", \"Basic\")))\n",
    "test_mnist(rnn_mnist, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac46ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mnist = Basic_LSTM_MNIST(input_size=28, out_dim=10).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_mnist(lstm_mnist, mnist_train_set, mnist_val_set, \"LSTM\", \"MNIST\",\"Basic\", epochs=80)\n",
    "lstm_mnist.load_state_dict(torch.load(get_save_path(\"LSTM\", \"MNIST\", \"Basic\")))\n",
    "test_mnist(lstm_mnist, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfe099",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_mnist = Basic_GRU_MNIST(input_size=28, out_dim=10).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_mnist(GRU_mnist, mnist_train_set, mnist_val_set, \"GRU\", \"MNIST\",\"Basic\")\n",
    "GRU_mnist.load_state_dict(torch.load(get_save_path(\"GRU\", \"MNIST\", \"Basic\")))\n",
    "test_mnist(GRU_mnist, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27931262",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hymmn0s_rnn_mnist = Hymmn0s_RNN_MNIST(input_size=28, hiddin_dim=24, out_dim=10, num_layers=5).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_mnist(Hymmn0s_rnn_mnist, mnist_train_set, mnist_val_set, \"RNN\", \"MNIST\",\"Hymmn0s\")\n",
    "Hymmn0s_rnn_mnist.load_state_dict(torch.load(get_save_path(\"RNN\", \"MNIST\", \"Hymmn0s\")))\n",
    "test_mnist(Hymmn0s_rnn_mnist, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hymmn0s_lstm_mnist = Hymmn0s_LSTM_MNIST(input_size=28, hiddin_dim=24, out_dim=10, num_layers=5).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_mnist(Hymmn0s_lstm_mnist, mnist_train_set, mnist_val_set, \"LSTM\", \"MNIST\",\"Hymmn0s\")\n",
    "Hymmn0s_lstm_mnist.load_state_dict(torch.load(get_save_path(\"LSTM\", \"MNIST\", \"Hymmn0s\")))\n",
    "test_mnist(Hymmn0s_lstm_mnist, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67698c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hymmn0s_gru_mnist = Hymmn0s_GRU_MNIST(input_size=28, hiddin_dim=24, out_dim=10, num_layers=5).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_mnist(Hymmn0s_gru_mnist, mnist_train_set, mnist_val_set, \"GRU\", \"MNIST\",\"Hymmn0s\")\n",
    "Hymmn0s_gru_mnist.load_state_dict(torch.load(get_save_path(\"GRU\", \"MNIST\", \"Hymmn0s\")))\n",
    "test_mnist(Hymmn0s_gru_mnist, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229dad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "embedding_dim = 64\n",
    "padding_token_id = vocab_size\n",
    "\n",
    "imdb_train = IMDBDataset(tokenized_train_comments, train_sentiments, train_scores)\n",
    "imdb_test = IMDBDataset(tokenized_test_comments, test_sentiments, test_scores)\n",
    "\n",
    "train_size = int(0.9 * len(imdb_train))\n",
    "val_size = len(imdb_train) - train_size\n",
    "imdb_train_set, imdb_val_set = random_split(imdb_train, [train_size, val_size])\n",
    "\n",
    "sample = imdb_train[0]\n",
    "sample_sentiment = imdb_train.sentiments[0]\n",
    "sample_score = imdb_train.scores[0]\n",
    "\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Sentiment: {sample_sentiment}\")\n",
    "print(f\"Score: {sample_score}\")\n",
    "print(f\"Reconstructed: {tokenizer.decode(sample[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_RNN_IMDB(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_x):\n",
    "        packed_output, hn = self.rnn(packed_x) \n",
    "\n",
    "        out = self.fc(hn.squeeze(0))\n",
    "        return out\n",
    "\n",
    "class Basic_LSTM_IMDB(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_x):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_x)\n",
    "        out = self.fc(hn.squeeze(0))\n",
    "        return out\n",
    "\n",
    "class Basic_GRU_IMDB(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_x):\n",
    "        packed_output, hn = self.gru(packed_x)\n",
    "        out = self.fc(hn.squeeze(0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hymmn0s_RNN_IMDB(nn.Module):\n",
    "    def __init__(self, vocab_size=50257, embedding_dim=64, hidden_dim=32, output_dim=1, num_layers=5, padding_idx=50257):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text_token_ids, text_lengths):\n",
    "        embedded_seqs = self.embedding(text_token_ids)\n",
    "        packed_input = pack_padded_sequence(embedded_seqs, text_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        packed_output, hn = self.rnn(packed_input)\n",
    "\n",
    "        out = self.fc(hn[-1, :, :])\n",
    "        return out\n",
    "    \n",
    "class Hymmn0s_LSTM_IMDB(nn.Module):\n",
    "    def __init__(self, vocab_size=50257, embedding_dim=64, hidden_dim=32, output_dim=1, num_layers=5, padding_idx=50257):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text_token_ids, text_lengths):\n",
    "        embedded_seqs = self.embedding(text_token_ids)\n",
    "        packed_input = pack_padded_sequence(embedded_seqs, text_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        out = self.fc(hn[-1, :, :])\n",
    "        return out\n",
    "    \n",
    "class Hymmn0s_GRU_IMDB(nn.Module):\n",
    "    def __init__(self, vocab_size=50257, embedding_dim=64, hidden_dim=32, output_dim=1, num_layers=5, padding_idx=50257):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=padding_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text_token_ids, text_lengths):\n",
    "        embedded_seqs = self.embedding(text_token_ids)\n",
    "        packed_input = pack_padded_sequence(embedded_seqs, text_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        packed_output, hn = self.gru(packed_input)\n",
    "\n",
    "        out = self.fc(hn[-1, :, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_from_score(score_tensor, threshold=5.0):\n",
    "    return (score_tensor > threshold).float()\n",
    "\n",
    "def train_imdb(model, train_dataset_obj, val_dataset_obj, model_name, dataset_name, model_type,\n",
    "               epochs=50, use_embedding=True, padding_token_id=50257, lr=2e-3, batch_size_param=256, weight_decay=2e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    regression_loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_loader = create_IMDB_dataloader(train_dataset_obj, batch_size=batch_size_param, shuffle=True, use_embedding=use_embedding, pad_value=padding_token_id)\n",
    "    val_loader = create_IMDB_dataloader(val_dataset_obj, batch_size=batch_size_param, shuffle=False, use_embedding=use_embedding, pad_value=padding_token_id)\n",
    "\n",
    "    best_val_mae = float('inf')\n",
    "    best_val_sentiment_accuracy = 0.0\n",
    "    model_save_path = get_save_path(model_name, dataset_name, model_type)\n",
    "    log_file_path = f\"log/{model_name}_on_{dataset_name}_{model_type}_train_log.csv\"\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss_regr = 0\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\")):\n",
    "            input_seq, lengths_or_none, true_scores, true_sentiments = None, None, None, None\n",
    "            if use_embedding:\n",
    "                input_seq, lengths_or_none, true_scores, true_sentiments = batch_data\n",
    "                input_seq = input_seq.to(device)\n",
    "            else:\n",
    "                input_seq, true_scores, true_sentiments = batch_data\n",
    "                input_seq = input_seq.to(device)\n",
    "\n",
    "            true_scores = true_scores.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_embedding and lengths_or_none is not None:\n",
    "                predicted_scores = model(input_seq, lengths_or_none)\n",
    "            else:\n",
    "                predicted_scores = model(input_seq)\n",
    "\n",
    "            if predicted_scores.ndim == 1:\n",
    "                predicted_scores = predicted_scores.unsqueeze(1)\n",
    "\n",
    "            loss_regr = regression_loss_fn(predicted_scores, true_scores)\n",
    "            loss_regr.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss_regr += loss_regr.item()\n",
    "\n",
    "        avg_train_loss_regr = total_train_loss_regr / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Regression Train Loss (MSE): {avg_train_loss_regr:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss_regr = 0\n",
    "        total_val_mae = 0\n",
    "        correct_sentiments = 0\n",
    "        total_sentiments_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data_val in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\"):\n",
    "                input_seq_val, lengths_val_or_none, true_scores_val, true_sentiments_val = None, None, None, None\n",
    "                if use_embedding:\n",
    "                    input_seq_val, lengths_val_or_none, true_scores_val, true_sentiments_val = batch_data_val\n",
    "                    input_seq_val = input_seq_val.to(device)\n",
    "                else:\n",
    "                    input_seq_val, true_scores_val, true_sentiments_val = batch_data_val\n",
    "                    input_seq_val = input_seq_val.to(device)\n",
    "\n",
    "                true_scores_val = true_scores_val.to(device).float().unsqueeze(1)\n",
    "                true_sentiments_val = true_sentiments_val.to(device).float()\n",
    "\n",
    "\n",
    "                if use_embedding and lengths_val_or_none is not None:\n",
    "                    predicted_scores_val = model(input_seq_val, lengths_val_or_none)\n",
    "                else:\n",
    "                    predicted_scores_val = model(input_seq_val)\n",
    "\n",
    "                if predicted_scores_val.ndim == 1:\n",
    "                    predicted_scores_val = predicted_scores_val.unsqueeze(1)\n",
    "\n",
    "                val_loss_regr_batch = regression_loss_fn(predicted_scores_val, true_scores_val)\n",
    "                total_val_loss_regr += val_loss_regr_batch.item()\n",
    "                total_val_mae += nn.functional.l1_loss(predicted_scores_val, true_scores_val, reduction=\"sum\").item()\n",
    "\n",
    "                predicted_sentiments = predict_sentiment_from_score(predicted_scores_val.squeeze())\n",
    "                correct_sentiments += (predicted_sentiments == true_sentiments_val).sum().item()\n",
    "                total_sentiments_samples += true_sentiments_val.size(0)\n",
    "\n",
    "\n",
    "        avg_val_loss_regr = total_val_loss_regr / len(val_loader)\n",
    "        avg_val_mae = total_val_mae / total_sentiments_samples if total_sentiments_samples > 0 else 0\n",
    "        val_sentiment_accuracy = correct_sentiments / total_sentiments_samples if total_sentiments_samples > 0 else 0\n",
    "\n",
    "        print(f\"Validation MSE: {avg_val_loss_regr:.4f}, Validation MAE: {avg_val_mae:.4f}, Validation Sentiment Acc: {val_sentiment_accuracy*100:.2f}%\")\n",
    "        append_metrics_to_log(log_file_path, epoch + 1, avg_train_loss_regr, avg_val_loss_regr, val_accuracy=val_sentiment_accuracy, val_mae=avg_val_mae)\n",
    "\n",
    "        if avg_val_mae < best_val_mae:\n",
    "            best_val_mae = avg_val_mae\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"New best model (by MAE) saved with Val MAE: {best_val_mae:.4f}, Sentiment Acc: {val_sentiment_accuracy*100:.2f}%\")\n",
    "        if val_sentiment_accuracy > best_val_sentiment_accuracy :\n",
    "             best_val_sentiment_accuracy = val_sentiment_accuracy\n",
    "\n",
    "def test_imdb(model, test_dataset_obj, use_embedding=True, threshold=5.0, padding_token_id=50257, batch_size_param=256):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    test_loader = create_IMDB_dataloader(test_dataset_obj, batch_size=batch_size_param, shuffle=False, use_embedding=use_embedding, pad_value=padding_token_id)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_mae_sum = 0\n",
    "    correct_sentiments = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data_test in tqdm(test_loader, desc=\"Testing IMDB\"):\n",
    "            input_seq_test, lengths_test_or_none, true_scores_test, true_sentiments_test = None, None, None, None\n",
    "            if use_embedding:\n",
    "                input_seq_test, lengths_test_or_none, true_scores_test, true_sentiments_test = batch_data_test\n",
    "                input_seq_test = input_seq_test.to(device)\n",
    "            else:\n",
    "                input_seq_test, true_scores_test, true_sentiments_test = batch_data_test\n",
    "                input_seq_test = input_seq_test.to(device)\n",
    "\n",
    "            true_scores_test = true_scores_test.to(device).float().unsqueeze(1)\n",
    "            true_sentiments_test = true_sentiments_test.to(device).float()\n",
    "\n",
    "\n",
    "            if use_embedding and lengths_test_or_none is not None:\n",
    "                predicted_scores_test = model(input_seq_test, lengths_test_or_none)\n",
    "            else:\n",
    "                predicted_scores_test = model(input_seq_test)\n",
    "\n",
    "            if predicted_scores_test.ndim == 1:\n",
    "                predicted_scores_test = predicted_scores_test.unsqueeze(1)\n",
    "\n",
    "            total_mae_sum += nn.functional.l1_loss(predicted_scores_test, true_scores_test, reduction='sum').item()\n",
    "\n",
    "            predicted_sentiments_test = predict_sentiment_from_score(predicted_scores_test.squeeze(), threshold)\n",
    "            correct_sentiments += (predicted_sentiments_test == true_sentiments_test).sum().item()\n",
    "            total_samples += true_sentiments_test.size(0)\n",
    "\n",
    "    average_mae = total_mae_sum / total_samples if total_samples > 0 else 0\n",
    "    sentiment_accuracy = correct_sentiments / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    print(f\"Test MAE (Average Absolute Error): {average_mae:.4f}\")\n",
    "    print(f\"Test Sentiment Classification Accuracy (Threshold > {threshold}): {sentiment_accuracy*100:.2f}%\")\n",
    "\n",
    "    #return average_mae, sentiment_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_imdb = Basic_RNN_IMDB(input_dim=1, hidden_dim=32, output_dim=1, num_layers=1).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_imdb(rnn_imdb, imdb_train_set, imdb_val_set, \"RNN\", \"IMDB\",\"Basic\", use_embedding=False)\n",
    "rnn_imdb.load_state_dict(torch.load(get_save_path(\"RNN\", \"IMDB\", \"Basic\")))\n",
    "test_imdb(rnn_imdb, imdb_test, use_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a569f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_imdb = Basic_LSTM_IMDB(input_dim=1, hidden_dim=32, output_dim=1, num_layers=1).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_imdb(lstm_imdb, imdb_train_set, imdb_val_set, \"LSTM\", \"IMDB\",\"Basic\", use_embedding=False)\n",
    "lstm_imdb.load_state_dict(torch.load(get_save_path(\"LSTM\", \"IMDB\", \"Basic\")))\n",
    "test_imdb(lstm_imdb, imdb_test, use_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_imdb = Basic_GRU_IMDB(input_dim=1, hidden_dim=32, output_dim=1, num_layers=1).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_imdb(gru_imdb, imdb_train_set, imdb_val_set, \"GRU\", \"IMDB\",\"Basic\", use_embedding=False)\n",
    "gru_imdb.load_state_dict(torch.load(get_save_path(\"GRU\", \"IMDB\", \"Basic\")))\n",
    "test_imdb(gru_imdb, imdb_test, use_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hymmn0s_rnn_imdb = Hymmn0s_RNN_IMDB(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=32, output_dim=1, num_layers=3).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_imdb(Hymmn0s_rnn_imdb, imdb_train_set, imdb_val_set, \"RNN\", \"IMDB\",\"Hymmn0s\", use_embedding=True)\n",
    "Hymmn0s_rnn_imdb.load_state_dict(torch.load(get_save_path(\"RNN\", \"IMDB\", \"Hymmn0s\")))\n",
    "test_imdb(Hymmn0s_rnn_imdb, imdb_test, use_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96902e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hymmn0s_lstm_imdb = Hymmn0s_LSTM_IMDB(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=32, output_dim=1, num_layers=3).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_imdb(Hymmn0s_lstm_imdb, imdb_train_set, imdb_val_set, \"LSTM\", \"IMDB\",\"Hymmn0s\", use_embedding=True)\n",
    "Hymmn0s_lstm_imdb.load_state_dict(torch.load(get_save_path(\"LSTM\", \"IMDB\", \"Hymmn0s\")))\n",
    "test_imdb(Hymmn0s_lstm_imdb, imdb_test, use_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hymmn0s_gru_imdb = Hymmn0s_GRU_IMDB(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=32, output_dim=1, num_layers=3).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#train_imdb(Hymmn0s_gru_imdb, imdb_train_set, imdb_val_set, \"GRU\", \"IMDB\",\"Hymmn0s\", use_embedding=True)\n",
    "Hymmn0s_gru_imdb.load_state_dict(torch.load(get_save_path(\"GRU\", \"IMDB\", \"Hymmn0s\")))\n",
    "test_imdb(Hymmn0s_gru_imdb, imdb_test, use_embedding=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
